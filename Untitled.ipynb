{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/maksym/anaconda3/envs/smse/lib/python3.6/site-packages/sklearn/utils/linear_assignment_.py:21: DeprecationWarning: The linear_assignment_ module is deprecated in 0.21 and will be removed from 0.23. Use scipy.optimize.linear_sum_assignment instead.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import pytest\n",
    "\n",
    "from align.data.xnli_reader import XnliReader\n",
    "from allennlp.common.util import ensure_list\n",
    "from allennlp.common.testing import AllenNlpTestCase\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.common.testing import ModelTestCase\n",
    "from allennlp.data.token_indexers.wordpiece_indexer import PretrainedBertIndexer, _get_token_type_ids\n",
    "from allennlp.data.tokenizers import WordTokenizer, Token\n",
    "from allennlp.data.tokenizers.word_splitter import BertBasicWordSplitter\n",
    "from allennlp.data.vocabulary import Vocabulary\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-f32015172f6d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m#                                          [CLS] 2, 15, 10, 11, 6, 17, 2  15, 10, 11, 6, [SEP]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0mindexed_tokens\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"bert-type-ids\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m    \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m#pylint: disable=bad-whitespace\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tokenizer = WordTokenizer()\n",
    "\n",
    "sentence = \"the laziest  fox\"\n",
    "\n",
    "tokens = tokenizer.tokenize(sentence)\n",
    "#           2   15 10 11  6   17    2   15 10 11  6\n",
    "#           the laziest   fox [SEP] the laziest   fox\n",
    "tokens = tokens + [Token(\"[SEP]\")] + tokens  # have to do this b/c tokenizer splits `[SEP]` in three\n",
    "\n",
    "vocab = Vocabulary()\n",
    "token_indexer = PretrainedBertIndexer('bert-base-multilingual-cased', do_lowercase=False)\n",
    "\n",
    "indexed_tokens = token_indexer.tokens_to_indices(tokens, vocab, \"bert\")\n",
    "\n",
    "#                                          [CLS] 2, 15, 10, 11, 6, 17, 2  15, 10, 11, 6, [SEP]\n",
    "assert indexed_tokens[\"bert-type-ids\"] == [0,    0, 0,  0,  0,  0, 0,  1, 1,  1,  1,  1, 1]  #pylint: disable=bad-whitespace\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bert': [101,\n",
       "  10105,\n",
       "  10109,\n",
       "  14548,\n",
       "  10562,\n",
       "  174,\n",
       "  31978,\n",
       "  102,\n",
       "  10105,\n",
       "  10109,\n",
       "  14548,\n",
       "  10562,\n",
       "  174,\n",
       "  31978,\n",
       "  102],\n",
       " 'bert-offsets': [1, 4, 6, 7, 8, 11, 13],\n",
       " 'bert-type-ids': [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1],\n",
       " 'mask': [1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indexed_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:smse]",
   "language": "python",
   "name": "conda-env-smse-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
